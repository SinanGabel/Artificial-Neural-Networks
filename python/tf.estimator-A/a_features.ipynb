{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/04_features/a_features.ipynb\n",
    "\n",
    "Nearly same source example: https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/05_artandscience/a_handtuning.ipynb\n",
    "\n",
    "I have made some changes to the original source notebook.\n",
    "\n",
    "You can run this notebook in Google Cloud Datalab.\n",
    "\n",
    "If you want to run this notebook in jupyter you may have to install google.datalab.ml for tensorboard, see how: https://github.com/googledatalab/pydatalab\n",
    "\n",
    "Versions: I have tried this notebook with Google Cloud Datalab with tensorflow 1.8, and jupyter with tensorflow 1.12.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Results will differ to run to run.\n",
    "\n",
    "Best result of a few different model and/or different parameter quick runs:\n",
    "\n",
    "estimator = tf.estimator.DNNRegressor(model_dir = output_dir, \n",
    "                                        feature_columns=create_feature_cols(), \n",
    "                                        hidden_units=[128, 64, 16], \n",
    "                                        activation_fn=tf.nn.tanh, \n",
    "                                        dropout=0.15)\n",
    "\n",
    "global step 10000: average_loss = 0.41720954, global_step = 10000, label/mean = 2.0454628, loss = 52.35207, prediction/mean = 2.2294657"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4f3CKqFUqL2-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Trying out features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "  * Improve the accuracy of a model by adding new features with the appropriate representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is based on 1990 census data from California. This data is at the city block level, so these features reflect the total number of rooms in that block, or the total number of people who live on that block, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TjLjL9IU80G"
   },
   "source": [
    "## Set Up\n",
    "In this first cell, we'll load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from google.datalab.ml import TensorBoard\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipRyUHjhU80Q"
   },
   "source": [
    "Next, we'll load our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")\n",
    "\n",
    "df = pd.read_csv(\"data/california_housing_train.csv\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HzzlSs3PtTmt",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Examine and split the data\n",
    "\n",
    "It's a good idea to get to know your data a little bit before you work with it.\n",
    "\n",
    "We'll print out a quick summary of a few useful statistics on each column.\n",
    "\n",
    "This will include things like mean, standard deviation, max, min, and various quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-114.3</td>\n",
       "      <td>34.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5612.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>66900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-114.5</td>\n",
       "      <td>34.4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7650.0</td>\n",
       "      <td>1901.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>80100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-114.6</td>\n",
       "      <td>33.7</td>\n",
       "      <td>17.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>85700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-114.6</td>\n",
       "      <td>33.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>73400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-114.6</td>\n",
       "      <td>33.6</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>65500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0     -114.3      34.2                15.0       5612.0          1283.0   \n",
       "1     -114.5      34.4                19.0       7650.0          1901.0   \n",
       "2     -114.6      33.7                17.0        720.0           174.0   \n",
       "3     -114.6      33.6                14.0       1501.0           337.0   \n",
       "4     -114.6      33.6                20.0       1454.0           326.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0      1015.0       472.0            1.5             66900.0  \n",
       "1      1129.0       463.0            1.8             80100.0  \n",
       "2       333.0       117.0            1.7             85700.0  \n",
       "3       515.0       226.0            3.2             73400.0  \n",
       "4       624.0       262.0            1.9             65500.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "test": {
      "output": "ignore",
      "timeout": 600
     }
    },
    "colab_type": "code",
    "id": "gzb10yoVrydW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.6</td>\n",
       "      <td>35.6</td>\n",
       "      <td>28.6</td>\n",
       "      <td>2643.7</td>\n",
       "      <td>539.4</td>\n",
       "      <td>1429.6</td>\n",
       "      <td>501.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>207300.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>2179.9</td>\n",
       "      <td>421.5</td>\n",
       "      <td>1147.9</td>\n",
       "      <td>384.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>115983.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.3</td>\n",
       "      <td>32.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1462.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>119400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.5</td>\n",
       "      <td>34.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2127.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>180400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3151.2</td>\n",
       "      <td>648.2</td>\n",
       "      <td>1721.0</td>\n",
       "      <td>605.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37937.0</td>\n",
       "      <td>6445.0</td>\n",
       "      <td>35682.0</td>\n",
       "      <td>6082.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>500001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "count    17000.0   17000.0             17000.0      17000.0         17000.0   \n",
       "mean      -119.6      35.6                28.6       2643.7           539.4   \n",
       "std          2.0       2.1                12.6       2179.9           421.5   \n",
       "min       -124.3      32.5                 1.0          2.0             1.0   \n",
       "25%       -121.8      33.9                18.0       1462.0           297.0   \n",
       "50%       -118.5      34.2                29.0       2127.0           434.0   \n",
       "75%       -118.0      37.7                37.0       3151.2           648.2   \n",
       "max       -114.3      42.0                52.0      37937.0          6445.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \n",
       "count     17000.0     17000.0        17000.0             17000.0  \n",
       "mean       1429.6       501.2            3.9            207300.9  \n",
       "std        1147.9       384.5            1.9            115983.8  \n",
       "min           3.0         1.0            0.5             14999.0  \n",
       "25%         790.0       282.0            2.6            119400.0  \n",
       "50%        1167.0       409.0            3.5            180400.0  \n",
       "75%        1721.0       605.2            4.8            265000.0  \n",
       "max       35682.0      6082.0           15.0            500001.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, split the data into two parts -- training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1) #makes result reproducible\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "traindf = df[msk]\n",
    "evaldf = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.6</td>\n",
       "      <td>35.6</td>\n",
       "      <td>28.7</td>\n",
       "      <td>2632.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>1423.3</td>\n",
       "      <td>498.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>207986.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>2163.3</td>\n",
       "      <td>416.7</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>379.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>116514.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.3</td>\n",
       "      <td>32.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>787.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>119600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.5</td>\n",
       "      <td>34.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2117.5</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>180800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3146.0</td>\n",
       "      <td>644.2</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>266300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37937.0</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>35682.0</td>\n",
       "      <td>5189.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>500001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "count    13612.0   13612.0             13612.0      13612.0         13612.0   \n",
       "mean      -119.6      35.6                28.7       2632.0           536.0   \n",
       "std          2.0       2.1                12.6       2163.3           416.7   \n",
       "min       -124.3      32.5                 1.0          8.0             1.0   \n",
       "25%       -121.8      33.9                18.0       1461.0           296.0   \n",
       "50%       -118.5      34.2                29.0       2117.5           432.0   \n",
       "75%       -118.0      37.7                37.0       3146.0           644.2   \n",
       "max       -114.3      42.0                52.0      37937.0          5471.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \n",
       "count     13612.0     13612.0        13612.0             13612.0  \n",
       "mean       1423.3       498.1            3.9            207986.5  \n",
       "std        1126.0       379.3            1.9            116514.3  \n",
       "min           3.0         1.0            0.5             14999.0  \n",
       "25%         787.0       281.0            2.6            119600.0  \n",
       "50%        1168.0       408.0            3.6            180800.0  \n",
       "75%        1715.0       602.0            4.8            266300.0  \n",
       "max       35682.0      5189.0           15.0            500001.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.6</td>\n",
       "      <td>35.7</td>\n",
       "      <td>28.3</td>\n",
       "      <td>2690.4</td>\n",
       "      <td>553.0</td>\n",
       "      <td>1454.8</td>\n",
       "      <td>513.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>204546.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>2245.5</td>\n",
       "      <td>440.2</td>\n",
       "      <td>1231.5</td>\n",
       "      <td>404.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>113802.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.3</td>\n",
       "      <td>32.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>283.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>118800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.6</td>\n",
       "      <td>34.3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2171.5</td>\n",
       "      <td>441.0</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>178650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3167.2</td>\n",
       "      <td>667.0</td>\n",
       "      <td>1756.2</td>\n",
       "      <td>615.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>258825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.6</td>\n",
       "      <td>41.9</td>\n",
       "      <td>52.0</td>\n",
       "      <td>32627.0</td>\n",
       "      <td>6445.0</td>\n",
       "      <td>28566.0</td>\n",
       "      <td>6082.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>500001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "count     3388.0    3388.0              3388.0       3388.0          3388.0   \n",
       "mean      -119.6      35.7                28.3       2690.4           553.0   \n",
       "std          2.0       2.1                12.6       2245.5           440.2   \n",
       "min       -124.3      32.5                 2.0          2.0             2.0   \n",
       "25%       -121.8      33.9                18.0       1467.0           300.0   \n",
       "50%       -118.6      34.3                28.0       2171.5           441.0   \n",
       "75%       -118.0      37.7                37.0       3167.2           667.0   \n",
       "max       -114.6      41.9                52.0      32627.0          6445.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \n",
       "count      3388.0      3388.0         3388.0              3388.0  \n",
       "mean       1454.8       513.7            3.8            204546.3  \n",
       "std        1231.5       404.7            1.8            113802.5  \n",
       "min           6.0         2.0            0.5             22500.0  \n",
       "25%         796.0       283.8            2.5            118800.0  \n",
       "50%        1160.0       414.0            3.5            178650.0  \n",
       "75%        1756.2       615.2            4.7            258825.0  \n",
       "max       28566.0      6082.0           15.0            500001.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaldf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "In this exercise, we'll be trying to predict **median_house_value** It will be our label (sometimes also called a target).\n",
    "\n",
    "We'll modify the feature_cols and input function to represent the features you want to use.\n",
    "\n",
    "Note: total_rooms is per block so to get rooms per house, and other, we need make some transformations.\n",
    "\n",
    "We divide **total_rooms** by **households** to get **avg_rooms_per_house** which we excect to positively correlate with **median_house_value**. \n",
    "\n",
    "We also divide **population** by **total_rooms** to get **avg_persons_per_room** which we expect to negatively correlate with **median_house_value**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_more_features(df):\n",
    "  df['avg_rooms_per_house'] = df['total_rooms'] / df['households'] # positive correlation\n",
    "  df['avg_bedrooms_per_house'] = df['total_bedrooms'] / df['households'] # positive correlation\n",
    "  df['avg_persons_per_room'] = df['population'] / df['total_rooms'] # negative correlation\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_more_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas input function: returns function, that has signature of () -> (dict of features, target)\n",
    "SCALE=100000\n",
    "BATCH_SIZE=128\n",
    "\n",
    "def make_input_fn(df, num_epochs):\n",
    "  return tf.estimator.inputs.pandas_input_fn(\n",
    "    x = add_more_features(df),\n",
    "    #x = df,  \n",
    "    y = df['median_house_value'] / SCALE, # will talk about why later in the course\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_epochs = num_epochs,\n",
    "    shuffle = True,\n",
    "    queue_capacity = 1000,\n",
    "    num_threads = 1\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your feature columns\n",
    "\n",
    "# np.arange() similar to np.linspace(), but uses a step size instead of the number of samples.\n",
    "\n",
    "def create_feature_cols():\n",
    "  return [\n",
    "    tf.feature_column.numeric_column('housing_median_age'),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('latitude'), boundaries = np.linspace(32.0, 42, num=10).tolist()),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('longitude'), boundaries = np.linspace(-124.3, -114.3, num=10).tolist()),\n",
    "    tf.feature_column.numeric_column('avg_rooms_per_house'),\n",
    "    tf.feature_column.numeric_column('avg_bedrooms_per_house'),\n",
    "    tf.feature_column.numeric_column('avg_persons_per_room'),\n",
    "    tf.feature_column.numeric_column('median_income')\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a model ...\n",
    "\n",
    "## LinearRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note LinearRegressor default loss: loss is calculated by using mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "  estimator = tf.estimator.LinearRegressor(model_dir = output_dir, feature_columns = create_feature_cols())\n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n",
    "                                      max_steps = num_train_steps)\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n",
    "                                    steps = None, \n",
    "                                    start_delay_secs = 1, # start evaluating after N seconds, \n",
    "                                    throttle_secs = 5)  # evaluate every N seconds\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "  estimator = tf.estimator.LinearRegressor(model_dir = output_dir, \n",
    "                                           feature_columns = create_feature_cols(),\n",
    "                                           optimizer=tf.train.FtrlOptimizer(learning_rate=0.1, l1_regularization_strength=0.001))\n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n",
    "                                      max_steps = num_train_steps)\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n",
    "                                    steps = None, \n",
    "                                    start_delay_secs = 1, # start evaluating after N seconds, \n",
    "                                    throttle_secs = 5)  # evaluate every N seconds\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNNRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note DNNRegressor default loss: loss is calculated by using mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "\n",
    "  estimator = tf.estimator.DNNRegressor(model_dir = output_dir, \n",
    "                                        feature_columns=create_feature_cols(), \n",
    "                                        hidden_units=[128, 64, 16], \n",
    "                                        activation_fn=tf.nn.tanh, \n",
    "                                        dropout=0.15)\n",
    "  \n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n",
    "                                      max_steps = num_train_steps)\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n",
    "                                    steps = None, \n",
    "                                    start_delay_secs = 1, # start evaluating after N seconds, \n",
    "                                    throttle_secs = 5)  # evaluate every N seconds\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "\n",
    "  estimator = tf.estimator.DNNRegressor(model_dir = output_dir, \n",
    "                                        feature_columns=create_feature_cols(), \n",
    "                                        hidden_units=[128, 64, 16], \n",
    "                                        activation_fn=tf.nn.tanh, \n",
    "                                        dropout=0.25,\n",
    "                                        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.1,l1_regularization_strength=0.001))\n",
    "  \n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n",
    "                                      max_steps = num_train_steps)\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n",
    "                                    steps = None, \n",
    "                                    start_delay_secs = 1, # start evaluating after N seconds, \n",
    "                                    throttle_secs = 5)  # evaluate every N seconds\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try add RMSE metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "\n",
    "  estimator = tf.estimator.DNNRegressor(model_dir = output_dir, \n",
    "                                        feature_columns=create_feature_cols(), \n",
    "                                        hidden_units=[128, 64, 16], \n",
    "                                        activation_fn=tf.nn.tanh, \n",
    "                                        dropout=0.25)\n",
    "\n",
    "  # --- Add RMSE evaluation metric: it is simply the square root of the default metric: MSE ---\n",
    "  def rmse(labels, predictions):\n",
    "    pred_values = tf.cast(predictions['predictions'], tf.float64)\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "  \n",
    "  estimator = tf.contrib.estimator.add_metrics(estimator,rmse)\n",
    "  \n",
    "  # --- continue ---\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n",
    "                                      max_steps = num_train_steps)\n",
    "\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n",
    "                                    steps = None, \n",
    "                                    start_delay_secs = 1, # start evaluating after N seconds, \n",
    "                                    throttle_secs = 5)  # evaluate every N seconds\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = './trained_model'\n",
    "\n",
    "try:\n",
    "  os.makedirs(OUTDIR)\n",
    "except OSError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './trained_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f58879c7fd0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 827.43524, step = 1\n",
      "INFO:tensorflow:global_step/sec: 176.036\n",
      "INFO:tensorflow:loss = 68.17198, step = 101 (0.569 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.58\n",
      "INFO:tensorflow:loss = 40.02434, step = 201 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.745\n",
      "INFO:tensorflow:loss = 75.79686, step = 301 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.13\n",
      "INFO:tensorflow:loss = 78.244545, step = 401 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.004\n",
      "INFO:tensorflow:loss = 57.22071, step = 501 (0.604 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.796\n",
      "INFO:tensorflow:loss = 44.42904, step = 601 (0.665 sec)\n",
      "INFO:tensorflow:global_step/sec: 137.614\n",
      "INFO:tensorflow:loss = 15.690523, step = 701 (0.743 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.746\n",
      "INFO:tensorflow:loss = 31.392548, step = 801 (0.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.952\n",
      "INFO:tensorflow:loss = 64.47227, step = 901 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.549\n",
      "INFO:tensorflow:loss = 78.48854, step = 1001 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.171\n",
      "INFO:tensorflow:loss = 138.20375, step = 1101 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.21\n",
      "INFO:tensorflow:loss = 106.89268, step = 1201 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.457\n",
      "INFO:tensorflow:loss = 44.180542, step = 1301 (0.608 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.025\n",
      "INFO:tensorflow:loss = 59.66532, step = 1401 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.191\n",
      "INFO:tensorflow:loss = 44.95234, step = 1501 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.87\n",
      "INFO:tensorflow:loss = 51.38823, step = 1601 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.923\n",
      "INFO:tensorflow:loss = 72.33876, step = 1701 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.082\n",
      "INFO:tensorflow:loss = 56.762444, step = 1801 (0.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.651\n",
      "INFO:tensorflow:loss = 43.875725, step = 1901 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.968\n",
      "INFO:tensorflow:loss = 130.98645, step = 2001 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.367\n",
      "INFO:tensorflow:loss = 45.258286, step = 2101 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.248\n",
      "INFO:tensorflow:loss = 51.73085, step = 2201 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.806\n",
      "INFO:tensorflow:loss = 46.25264, step = 2301 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.719\n",
      "INFO:tensorflow:loss = 23.80843, step = 2401 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.798\n",
      "INFO:tensorflow:loss = 40.791794, step = 2501 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.661\n",
      "INFO:tensorflow:loss = 81.0383, step = 2601 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.596\n",
      "INFO:tensorflow:loss = 54.080666, step = 2701 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.382\n",
      "INFO:tensorflow:loss = 97.728455, step = 2801 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.855\n",
      "INFO:tensorflow:loss = 79.69905, step = 2901 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.645\n",
      "INFO:tensorflow:loss = 53.423836, step = 3001 (0.656 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.163\n",
      "INFO:tensorflow:loss = 47.72063, step = 3101 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.203\n",
      "INFO:tensorflow:loss = 34.291218, step = 3201 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.829\n",
      "INFO:tensorflow:loss = 45.974457, step = 3301 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.111\n",
      "INFO:tensorflow:loss = 77.99597, step = 3401 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.826\n",
      "INFO:tensorflow:loss = 18.978575, step = 3501 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.346\n",
      "INFO:tensorflow:loss = 44.30119, step = 3601 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.271\n",
      "INFO:tensorflow:loss = 129.87477, step = 3701 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.26\n",
      "INFO:tensorflow:loss = 63.33138, step = 3801 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.859\n",
      "INFO:tensorflow:loss = 38.7442, step = 3901 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.378\n",
      "INFO:tensorflow:loss = 35.718594, step = 4001 (0.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.432\n",
      "INFO:tensorflow:loss = 22.230484, step = 4101 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.957\n",
      "INFO:tensorflow:loss = 107.46471, step = 4201 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.236\n",
      "INFO:tensorflow:loss = 56.838203, step = 4301 (0.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.243\n",
      "INFO:tensorflow:loss = 78.78877, step = 4401 (0.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.909\n",
      "INFO:tensorflow:loss = 97.36891, step = 4501 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.23\n",
      "INFO:tensorflow:loss = 91.32182, step = 4601 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.527\n",
      "INFO:tensorflow:loss = 51.48599, step = 4701 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.834\n",
      "INFO:tensorflow:loss = 37.16499, step = 4801 (0.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.329\n",
      "INFO:tensorflow:loss = 56.55757, step = 4901 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.31\n",
      "INFO:tensorflow:loss = 106.008514, step = 5001 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.113\n",
      "INFO:tensorflow:loss = 71.319565, step = 5101 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.814\n",
      "INFO:tensorflow:loss = 21.950087, step = 5201 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.931\n",
      "INFO:tensorflow:loss = 51.25126, step = 5301 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.092\n",
      "INFO:tensorflow:loss = 142.54768, step = 5401 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.386\n",
      "INFO:tensorflow:loss = 56.669228, step = 5501 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.342\n",
      "INFO:tensorflow:loss = 51.90468, step = 5601 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.601\n",
      "INFO:tensorflow:loss = 26.684914, step = 5701 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.239\n",
      "INFO:tensorflow:loss = 34.17191, step = 5801 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.81\n",
      "INFO:tensorflow:loss = 113.47119, step = 5901 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.257\n",
      "INFO:tensorflow:loss = 49.668335, step = 6001 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.181\n",
      "INFO:tensorflow:loss = 105.52922, step = 6101 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.062\n",
      "INFO:tensorflow:loss = 110.36314, step = 6201 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.418\n",
      "INFO:tensorflow:loss = 66.466576, step = 6301 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.937\n",
      "INFO:tensorflow:loss = 52.021996, step = 6401 (0.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.662\n",
      "INFO:tensorflow:loss = 82.794876, step = 6501 (0.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.198\n",
      "INFO:tensorflow:loss = 52.21106, step = 6601 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 75.75401, step = 6701 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.143\n",
      "INFO:tensorflow:loss = 68.36716, step = 6801 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.625\n",
      "INFO:tensorflow:loss = 36.78415, step = 6901 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.89\n",
      "INFO:tensorflow:loss = 70.02933, step = 7001 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.297\n",
      "INFO:tensorflow:loss = 87.96954, step = 7101 (0.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.309\n",
      "INFO:tensorflow:loss = 65.50078, step = 7201 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.014\n",
      "INFO:tensorflow:loss = 51.659542, step = 7301 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.607\n",
      "INFO:tensorflow:loss = 28.954227, step = 7401 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.333\n",
      "INFO:tensorflow:loss = 40.252655, step = 7501 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.36\n",
      "INFO:tensorflow:loss = 65.35426, step = 7601 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.19\n",
      "INFO:tensorflow:loss = 54.54081, step = 7701 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.278\n",
      "INFO:tensorflow:loss = 122.362076, step = 7801 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.421\n",
      "INFO:tensorflow:loss = 85.126976, step = 7901 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.823\n",
      "INFO:tensorflow:loss = 44.412994, step = 8001 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.893\n",
      "INFO:tensorflow:loss = 67.03168, step = 8101 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.433\n",
      "INFO:tensorflow:loss = 31.088236, step = 8201 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.873\n",
      "INFO:tensorflow:loss = 72.46942, step = 8301 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.369\n",
      "INFO:tensorflow:loss = 84.83243, step = 8401 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.517\n",
      "INFO:tensorflow:loss = 32.96861, step = 8501 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.495\n",
      "INFO:tensorflow:loss = 39.34855, step = 8601 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.278\n",
      "INFO:tensorflow:loss = 86.70674, step = 8701 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.773\n",
      "INFO:tensorflow:loss = 49.747917, step = 8801 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.327\n",
      "INFO:tensorflow:loss = 51.875923, step = 8901 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.183\n",
      "INFO:tensorflow:loss = 37.29435, step = 9001 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.447\n",
      "INFO:tensorflow:loss = 19.68467, step = 9101 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.319\n",
      "INFO:tensorflow:loss = 41.851498, step = 9201 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.234\n",
      "INFO:tensorflow:loss = 50.631344, step = 9301 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.859\n",
      "INFO:tensorflow:loss = 90.61564, step = 9401 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.638\n",
      "INFO:tensorflow:loss = 98.6194, step = 9501 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.234\n",
      "INFO:tensorflow:loss = 93.12683, step = 9601 (0.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.437\n",
      "INFO:tensorflow:loss = 55.845757, step = 9701 (0.501 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.3\n",
      "INFO:tensorflow:loss = 74.07033, step = 9801 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.849\n",
      "INFO:tensorflow:loss = 34.737667, step = 9901 (0.488 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into ./trained_model/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-22-10:10:05\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-22-10:10:05\n",
      "INFO:tensorflow:Saving dict for global step 10000: average_loss = 0.63636833, global_step = 10000, label/mean = 2.0454626, loss = 79.85244, prediction/mean = 1.7841936\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: ./trained_model/model.ckpt-10000\n",
      "INFO:tensorflow:Loss for final step: 70.85986.\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True)\n",
    "\n",
    "train_and_evaluate(OUTDIR, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 13369. Click <a href=\"/_proxy/59897/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "13369"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch tensorboard\n",
    "\n",
    "# Note: If you use jupyter then you may have to use the address shown in the terminal console instead of the below link to tensorboard.\n",
    "\n",
    "TensorBoard().start(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "first_steps_with_tensor_flow.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
