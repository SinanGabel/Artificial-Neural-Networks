{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/04_features/a_features.ipynb\n",
    "\n",
    "Nearly same source example: https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/05_artandscience/a_handtuning.ipynb\n",
    "\n",
    "I have made some changes to the original source notebook.\n",
    "\n",
    "You can run this notebook in Google Cloud Datalab.\n",
    "\n",
    "If you want to run this notebook in jupyter you may have to install google.datalab.ml for tensorboard, see how: https://github.com/googledatalab/pydatalab\n",
    "\n",
    "Versions: I have tried this notebook with Google Cloud Datalab with tensorflow 1.8, and jupyter with tensorflow 1.12.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Results will differ to run to run.\n",
    "\n",
    "Best result of a few different model and/or different parameter quick runs:\n",
    "\n",
    "estimator = tf.estimator.DNNRegressor(model_dir = output_dir, \n",
    "                                        feature_columns=create_feature_cols(), \n",
    "                                        hidden_units=[128, 64, 16], \n",
    "                                        activation_fn=tf.nn.tanh, \n",
    "                                        dropout=0.15)\n",
    "\n",
    "global step 10000: average_loss = 0.41720954, global_step = 10000, label/mean = 2.0454628, loss = 52.35207, prediction/mean = 2.2294657"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4f3CKqFUqL2-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Trying out features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "  * Improve the accuracy of a model by adding new features with the appropriate representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is based on 1990 census data from California. This data is at the city block level, so these features reflect the total number of rooms in that block, or the total number of people who live on that block, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TjLjL9IU80G"
   },
   "source": [
    "## Set Up\n",
    "In this first cell, we'll load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from google.datalab.ml import TensorBoard\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipRyUHjhU80Q"
   },
   "source": [
    "Next, we'll load our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")\n",
    "\n",
    "df = pd.read_csv(\"data/california_housing_train.csv\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HzzlSs3PtTmt",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Examine and split the data\n",
    "\n",
    "It's a good idea to get to know your data a little bit before you work with it.\n",
    "\n",
    "We'll print out a quick summary of a few useful statistics on each column.\n",
    "\n",
    "This will include things like mean, standard deviation, max, min, and various quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-114.3</td>\n",
       "      <td>34.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5612.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>66900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-114.5</td>\n",
       "      <td>34.4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7650.0</td>\n",
       "      <td>1901.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>80100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-114.6</td>\n",
       "      <td>33.7</td>\n",
       "      <td>17.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>85700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-114.6</td>\n",
       "      <td>33.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>73400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-114.6</td>\n",
       "      <td>33.6</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>65500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0     -114.3      34.2                15.0       5612.0          1283.0   \n",
       "1     -114.5      34.4                19.0       7650.0          1901.0   \n",
       "2     -114.6      33.7                17.0        720.0           174.0   \n",
       "3     -114.6      33.6                14.0       1501.0           337.0   \n",
       "4     -114.6      33.6                20.0       1454.0           326.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0      1015.0       472.0            1.5             66900.0  \n",
       "1      1129.0       463.0            1.8             80100.0  \n",
       "2       333.0       117.0            1.7             85700.0  \n",
       "3       515.0       226.0            3.2             73400.0  \n",
       "4       624.0       262.0            1.9             65500.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "test": {
      "output": "ignore",
      "timeout": 600
     }
    },
    "colab_type": "code",
    "id": "gzb10yoVrydW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.6</td>\n",
       "      <td>35.6</td>\n",
       "      <td>28.6</td>\n",
       "      <td>2643.7</td>\n",
       "      <td>539.4</td>\n",
       "      <td>1429.6</td>\n",
       "      <td>501.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>207300.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>2179.9</td>\n",
       "      <td>421.5</td>\n",
       "      <td>1147.9</td>\n",
       "      <td>384.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>115983.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.3</td>\n",
       "      <td>32.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1462.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>119400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.5</td>\n",
       "      <td>34.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2127.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>180400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3151.2</td>\n",
       "      <td>648.2</td>\n",
       "      <td>1721.0</td>\n",
       "      <td>605.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37937.0</td>\n",
       "      <td>6445.0</td>\n",
       "      <td>35682.0</td>\n",
       "      <td>6082.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>500001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "count    17000.0   17000.0             17000.0      17000.0         17000.0   \n",
       "mean      -119.6      35.6                28.6       2643.7           539.4   \n",
       "std          2.0       2.1                12.6       2179.9           421.5   \n",
       "min       -124.3      32.5                 1.0          2.0             1.0   \n",
       "25%       -121.8      33.9                18.0       1462.0           297.0   \n",
       "50%       -118.5      34.2                29.0       2127.0           434.0   \n",
       "75%       -118.0      37.7                37.0       3151.2           648.2   \n",
       "max       -114.3      42.0                52.0      37937.0          6445.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \n",
       "count     17000.0     17000.0        17000.0             17000.0  \n",
       "mean       1429.6       501.2            3.9            207300.9  \n",
       "std        1147.9       384.5            1.9            115983.8  \n",
       "min           3.0         1.0            0.5             14999.0  \n",
       "25%         790.0       282.0            2.6            119400.0  \n",
       "50%        1167.0       409.0            3.5            180400.0  \n",
       "75%        1721.0       605.2            4.8            265000.0  \n",
       "max       35682.0      6082.0           15.0            500001.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, split the data into two parts -- training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1) #makes result reproducible\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "traindf = df[msk]\n",
    "evaldf = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "      <td>13612.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.6</td>\n",
       "      <td>35.6</td>\n",
       "      <td>28.7</td>\n",
       "      <td>2632.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>1423.3</td>\n",
       "      <td>498.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>207986.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>2163.3</td>\n",
       "      <td>416.7</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>379.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>116514.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.3</td>\n",
       "      <td>32.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>787.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>119600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.5</td>\n",
       "      <td>34.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2117.5</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>180800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3146.0</td>\n",
       "      <td>644.2</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>266300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>37937.0</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>35682.0</td>\n",
       "      <td>5189.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>500001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "count    13612.0   13612.0             13612.0      13612.0         13612.0   \n",
       "mean      -119.6      35.6                28.7       2632.0           536.0   \n",
       "std          2.0       2.1                12.6       2163.3           416.7   \n",
       "min       -124.3      32.5                 1.0          8.0             1.0   \n",
       "25%       -121.8      33.9                18.0       1461.0           296.0   \n",
       "50%       -118.5      34.2                29.0       2117.5           432.0   \n",
       "75%       -118.0      37.7                37.0       3146.0           644.2   \n",
       "max       -114.3      42.0                52.0      37937.0          5471.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \n",
       "count     13612.0     13612.0        13612.0             13612.0  \n",
       "mean       1423.3       498.1            3.9            207986.5  \n",
       "std        1126.0       379.3            1.9            116514.3  \n",
       "min           3.0         1.0            0.5             14999.0  \n",
       "25%         787.0       281.0            2.6            119600.0  \n",
       "50%        1168.0       408.0            3.6            180800.0  \n",
       "75%        1715.0       602.0            4.8            266300.0  \n",
       "max       35682.0      5189.0           15.0            500001.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "      <td>3388.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.6</td>\n",
       "      <td>35.7</td>\n",
       "      <td>28.3</td>\n",
       "      <td>2690.4</td>\n",
       "      <td>553.0</td>\n",
       "      <td>1454.8</td>\n",
       "      <td>513.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>204546.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>2245.5</td>\n",
       "      <td>440.2</td>\n",
       "      <td>1231.5</td>\n",
       "      <td>404.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>113802.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.3</td>\n",
       "      <td>32.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>283.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>118800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.6</td>\n",
       "      <td>34.3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2171.5</td>\n",
       "      <td>441.0</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>178650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3167.2</td>\n",
       "      <td>667.0</td>\n",
       "      <td>1756.2</td>\n",
       "      <td>615.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>258825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.6</td>\n",
       "      <td>41.9</td>\n",
       "      <td>52.0</td>\n",
       "      <td>32627.0</td>\n",
       "      <td>6445.0</td>\n",
       "      <td>28566.0</td>\n",
       "      <td>6082.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>500001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "count     3388.0    3388.0              3388.0       3388.0          3388.0   \n",
       "mean      -119.6      35.7                28.3       2690.4           553.0   \n",
       "std          2.0       2.1                12.6       2245.5           440.2   \n",
       "min       -124.3      32.5                 2.0          2.0             2.0   \n",
       "25%       -121.8      33.9                18.0       1467.0           300.0   \n",
       "50%       -118.6      34.3                28.0       2171.5           441.0   \n",
       "75%       -118.0      37.7                37.0       3167.2           667.0   \n",
       "max       -114.6      41.9                52.0      32627.0          6445.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \n",
       "count      3388.0      3388.0         3388.0              3388.0  \n",
       "mean       1454.8       513.7            3.8            204546.3  \n",
       "std        1231.5       404.7            1.8            113802.5  \n",
       "min           6.0         2.0            0.5             22500.0  \n",
       "25%         796.0       283.8            2.5            118800.0  \n",
       "50%        1160.0       414.0            3.5            178650.0  \n",
       "75%        1756.2       615.2            4.7            258825.0  \n",
       "max       28566.0      6082.0           15.0            500001.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaldf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "In this exercise, we'll be trying to predict **median_house_value** It will be our label (sometimes also called a target).\n",
    "\n",
    "We'll modify the feature_cols and input function to represent the features you want to use.\n",
    "\n",
    "Note: total_rooms is per block so to get rooms per house, and other, we need make some transformations.\n",
    "\n",
    "We divide **total_rooms** by **households** to get **avg_rooms_per_house** which we excect to positively correlate with **median_house_value**. \n",
    "\n",
    "We also divide **population** by **total_rooms** to get **avg_persons_per_room** which we expect to negatively correlate with **median_house_value**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_more_features(df):\n",
    "  df['avg_rooms_per_house'] = df['total_rooms'] / df['households'] # positive correlation\n",
    "  df['avg_bedrooms_per_house'] = df['total_bedrooms'] / df['households'] # positive correlation\n",
    "  df['avg_persons_per_room'] = df['population'] / df['total_rooms'] # negative correlation\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_more_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas input function: returns function, that has signature of () -> (dict of features, target)\n",
    "SCALE=100000\n",
    "BATCH_SIZE=128\n",
    "\n",
    "def make_input_fn(df, num_epochs):\n",
    "  return tf.estimator.inputs.pandas_input_fn(\n",
    "    x = add_more_features(df),\n",
    "    #x = df,  \n",
    "    y = df['median_house_value'] / SCALE, # will talk about why later in the course\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_epochs = num_epochs,\n",
    "    shuffle = True,\n",
    "    queue_capacity = 1000,\n",
    "    num_threads = 1\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your feature columns\n",
    "\n",
    "# np.arange() similar to np.linspace(), but uses a step size instead of the number of samples.\n",
    "\n",
    "def create_feature_cols():\n",
    "  return [\n",
    "    tf.feature_column.numeric_column('housing_median_age'),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('latitude'), boundaries = np.linspace(32.0, 42, num=10).tolist()),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('longitude'), boundaries = np.linspace(-124.3, -114.3, num=10).tolist()),\n",
    "    tf.feature_column.numeric_column('avg_rooms_per_house'),\n",
    "    tf.feature_column.numeric_column('avg_bedrooms_per_house'),\n",
    "    tf.feature_column.numeric_column('avg_persons_per_room'),\n",
    "    tf.feature_column.numeric_column('median_income')\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a model ...\n",
    "\n",
    "## LinearRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note LinearRegressor default loss: loss is calculated by using mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "  estimator = tf.estimator.LinearRegressor(model_dir = output_dir, feature_columns = create_feature_cols())\n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n",
    "                                      max_steps = num_train_steps)\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n",
    "                                    steps = None, \n",
    "                                    start_delay_secs = 1, # start evaluating after N seconds, \n",
    "                                    throttle_secs = 5)  # evaluate every N seconds\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "  estimator = tf.estimator.LinearRegressor(model_dir = output_dir, \n",
    "                                           feature_columns = create_feature_cols(),\n",
    "                                           optimizer=tf.train.FtrlOptimizer(learning_rate=0.1, l1_regularization_strength=0.001))\n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n",
    "                                      max_steps = num_train_steps)\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n",
    "                                    steps = None, \n",
    "                                    start_delay_secs = 1, # start evaluating after N seconds, \n",
    "                                    throttle_secs = 5)  # evaluate every N seconds\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNNRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note DNNRegressor default loss: loss is calculated by using mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "\n",
    "  estimator = tf.estimator.DNNRegressor(model_dir = output_dir, \n",
    "                                        feature_columns=create_feature_cols(), \n",
    "                                        hidden_units=[128, 64, 16], \n",
    "                                        activation_fn=tf.nn.tanh, \n",
    "                                        dropout=0.15)\n",
    "  \n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n",
    "                                      max_steps = num_train_steps)\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n",
    "                                    steps = None, \n",
    "                                    start_delay_secs = 1, # start evaluating after N seconds, \n",
    "                                    throttle_secs = 5)  # evaluate every N seconds\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "\n",
    "  estimator = tf.estimator.DNNRegressor(model_dir = output_dir, \n",
    "                                        feature_columns=create_feature_cols(), \n",
    "                                        hidden_units=[128, 64, 16], \n",
    "                                        activation_fn=tf.nn.tanh, \n",
    "                                        dropout=0.25,\n",
    "                                        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.1,l1_regularization_strength=0.001))\n",
    "  \n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n",
    "                                      max_steps = num_train_steps)\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n",
    "                                    steps = None, \n",
    "                                    start_delay_secs = 1, # start evaluating after N seconds, \n",
    "                                    throttle_secs = 5)  # evaluate every N seconds\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try add RMSE metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator train and evaluate function\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "\n",
    "  estimator = tf.estimator.DNNRegressor(model_dir = output_dir, \n",
    "                                        feature_columns=create_feature_cols(), \n",
    "                                        hidden_units=[128, 64, 16], \n",
    "                                        activation_fn=tf.nn.tanh, \n",
    "                                        dropout=0.25)\n",
    "\n",
    "  # --- Add RMSE evaluation metric: it is simply the square root of the default metric: MSE ---\n",
    "  def rmse(labels, predictions):\n",
    "    pred_values = tf.cast(predictions['predictions'], tf.float64)\n",
    "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "  \n",
    "  estimator = tf.contrib.estimator.add_metrics(estimator,rmse)\n",
    "  \n",
    "  # --- continue ---\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \n",
    "                                      max_steps = num_train_steps)\n",
    "\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \n",
    "                                    steps = None, \n",
    "                                    start_delay_secs = 1, # start evaluating after N seconds, \n",
    "                                    throttle_secs = 5)  # evaluate every N seconds\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = './trained_model'\n",
    "\n",
    "try:\n",
    "  os.makedirs(OUTDIR)\n",
    "except OSError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './trained_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2583ac7c88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 338.76926, step = 1\n",
      "INFO:tensorflow:global_step/sec: 199.52\n",
      "INFO:tensorflow:loss = 185.88037, step = 101 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.065\n",
      "INFO:tensorflow:loss = 158.49567, step = 201 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.065\n",
      "INFO:tensorflow:loss = 119.17706, step = 301 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.785\n",
      "INFO:tensorflow:loss = 60.853485, step = 401 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.46\n",
      "INFO:tensorflow:loss = 95.09137, step = 501 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.219\n",
      "INFO:tensorflow:loss = 58.766685, step = 601 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.791\n",
      "INFO:tensorflow:loss = 80.36949, step = 701 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.776\n",
      "INFO:tensorflow:loss = 69.19928, step = 801 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.175\n",
      "INFO:tensorflow:loss = 51.652065, step = 901 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.067\n",
      "INFO:tensorflow:loss = 50.28682, step = 1001 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.529\n",
      "INFO:tensorflow:loss = 121.9766, step = 1101 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.094\n",
      "INFO:tensorflow:loss = 69.48098, step = 1201 (0.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.033\n",
      "INFO:tensorflow:loss = 85.64334, step = 1301 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.403\n",
      "INFO:tensorflow:loss = 52.018032, step = 1401 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.227\n",
      "INFO:tensorflow:loss = 39.709423, step = 1501 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.34\n",
      "INFO:tensorflow:loss = 46.715393, step = 1601 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.759\n",
      "INFO:tensorflow:loss = 97.88968, step = 1701 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.66\n",
      "INFO:tensorflow:loss = 89.08581, step = 1801 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.113\n",
      "INFO:tensorflow:loss = 109.38544, step = 1901 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.569\n",
      "INFO:tensorflow:loss = 136.07402, step = 2001 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.127\n",
      "INFO:tensorflow:loss = 51.780243, step = 2101 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.467\n",
      "INFO:tensorflow:loss = 48.42681, step = 2201 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.492\n",
      "INFO:tensorflow:loss = 34.538177, step = 2301 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.168\n",
      "INFO:tensorflow:loss = 81.48996, step = 2401 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.653\n",
      "INFO:tensorflow:loss = 83.63475, step = 2501 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.348\n",
      "INFO:tensorflow:loss = 42.23139, step = 2601 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.891\n",
      "INFO:tensorflow:loss = 48.952896, step = 2701 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.705\n",
      "INFO:tensorflow:loss = 91.90734, step = 2801 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.046\n",
      "INFO:tensorflow:loss = 73.7024, step = 2901 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.756\n",
      "INFO:tensorflow:loss = 60.607895, step = 3001 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.765\n",
      "INFO:tensorflow:loss = 44.442585, step = 3101 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.756\n",
      "INFO:tensorflow:loss = 36.391148, step = 3201 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.927\n",
      "INFO:tensorflow:loss = 49.45256, step = 3301 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.723\n",
      "INFO:tensorflow:loss = 43.342228, step = 3401 (0.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.684\n",
      "INFO:tensorflow:loss = 99.012314, step = 3501 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.599\n",
      "INFO:tensorflow:loss = 108.05066, step = 3601 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.872\n",
      "INFO:tensorflow:loss = 56.27343, step = 3701 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.875\n",
      "INFO:tensorflow:loss = 28.32977, step = 3801 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.22\n",
      "INFO:tensorflow:loss = 36.71074, step = 3901 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.175\n",
      "INFO:tensorflow:loss = 68.80999, step = 4001 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.753\n",
      "INFO:tensorflow:loss = 82.872696, step = 4101 (0.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.825\n",
      "INFO:tensorflow:loss = 90.17856, step = 4201 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.412\n",
      "INFO:tensorflow:loss = 52.87143, step = 4301 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.346\n",
      "INFO:tensorflow:loss = 61.989517, step = 4401 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.349\n",
      "INFO:tensorflow:loss = 96.03117, step = 4501 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.828\n",
      "INFO:tensorflow:loss = 51.004745, step = 4601 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.623\n",
      "INFO:tensorflow:loss = 53.78282, step = 4701 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.939\n",
      "INFO:tensorflow:loss = 49.835, step = 4801 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.164\n",
      "INFO:tensorflow:loss = 31.815487, step = 4901 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.416\n",
      "INFO:tensorflow:loss = 47.997654, step = 5001 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.767\n",
      "INFO:tensorflow:loss = 37.418945, step = 5101 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.645\n",
      "INFO:tensorflow:loss = 99.18669, step = 5201 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.831\n",
      "INFO:tensorflow:loss = 64.079475, step = 5301 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.135\n",
      "INFO:tensorflow:loss = 38.714096, step = 5401 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.805\n",
      "INFO:tensorflow:loss = 50.10058, step = 5501 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.046\n",
      "INFO:tensorflow:loss = 57.0414, step = 5601 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.618\n",
      "INFO:tensorflow:loss = 71.997635, step = 5701 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.291\n",
      "INFO:tensorflow:loss = 65.398705, step = 5801 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.724\n",
      "INFO:tensorflow:loss = 66.39638, step = 5901 (0.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.987\n",
      "INFO:tensorflow:loss = 40.802345, step = 6001 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.582\n",
      "INFO:tensorflow:loss = 80.05383, step = 6101 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.329\n",
      "INFO:tensorflow:loss = 65.395256, step = 6201 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.058\n",
      "INFO:tensorflow:loss = 38.394135, step = 6301 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.624\n",
      "INFO:tensorflow:loss = 42.883057, step = 6401 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.676\n",
      "INFO:tensorflow:loss = 24.109661, step = 6501 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.398\n",
      "INFO:tensorflow:loss = 39.202644, step = 6601 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 37.670395, step = 6701 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.371\n",
      "INFO:tensorflow:loss = 55.90632, step = 6801 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.85\n",
      "INFO:tensorflow:loss = 82.52406, step = 6901 (0.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.077\n",
      "INFO:tensorflow:loss = 58.363945, step = 7001 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.576\n",
      "INFO:tensorflow:loss = 49.954376, step = 7101 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.336\n",
      "INFO:tensorflow:loss = 77.60665, step = 7201 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.426\n",
      "INFO:tensorflow:loss = 40.904007, step = 7301 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.261\n",
      "INFO:tensorflow:loss = 64.83626, step = 7401 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.669\n",
      "INFO:tensorflow:loss = 46.562035, step = 7501 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.627\n",
      "INFO:tensorflow:loss = 46.480667, step = 7601 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.001\n",
      "INFO:tensorflow:loss = 46.033104, step = 7701 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.62\n",
      "INFO:tensorflow:loss = 78.454704, step = 7801 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.286\n",
      "INFO:tensorflow:loss = 44.309826, step = 7901 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.643\n",
      "INFO:tensorflow:loss = 53.42577, step = 8001 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.638\n",
      "INFO:tensorflow:loss = 38.707573, step = 8101 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.975\n",
      "INFO:tensorflow:loss = 20.704803, step = 8201 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.161\n",
      "INFO:tensorflow:loss = 43.62609, step = 8301 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.937\n",
      "INFO:tensorflow:loss = 58.306084, step = 8401 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.416\n",
      "INFO:tensorflow:loss = 59.115677, step = 8501 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.133\n",
      "INFO:tensorflow:loss = 67.77675, step = 8601 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.168\n",
      "INFO:tensorflow:loss = 78.608475, step = 8701 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.986\n",
      "INFO:tensorflow:loss = 24.012852, step = 8801 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.446\n",
      "INFO:tensorflow:loss = 43.69374, step = 8901 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.968\n",
      "INFO:tensorflow:loss = 35.916588, step = 9001 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.055\n",
      "INFO:tensorflow:loss = 67.663414, step = 9101 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.034\n",
      "INFO:tensorflow:loss = 48.12755, step = 9201 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.407\n",
      "INFO:tensorflow:loss = 52.23057, step = 9301 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.807\n",
      "INFO:tensorflow:loss = 41.980022, step = 9401 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.874\n",
      "INFO:tensorflow:loss = 77.01313, step = 9501 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.51\n",
      "INFO:tensorflow:loss = 71.563065, step = 9601 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.185\n",
      "INFO:tensorflow:loss = 40.632095, step = 9701 (0.450 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.09\n",
      "INFO:tensorflow:loss = 52.720306, step = 9801 (0.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.8\n",
      "INFO:tensorflow:loss = 39.568928, step = 9901 (0.471 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into ./trained_model/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-24-12:02:57\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-24-12:02:58\n",
      "INFO:tensorflow:Saving dict for global step 10000: average_loss = 0.41720954, global_step = 10000, label/mean = 2.0454628, loss = 52.35207, prediction/mean = 2.2294657\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: ./trained_model/model.ckpt-10000\n",
      "INFO:tensorflow:Loss for final step: 38.36551.\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True)\n",
    "\n",
    "train_and_evaluate(OUTDIR, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 18179. Click <a href=\"/_proxy/34675/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "18179"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch tensorboard\n",
    "\n",
    "# Note: If you use jupyter then you may have to use the address shown in the terminal console instead of the below link to tensorboard.\n",
    "\n",
    "TensorBoard().start(OUTDIR)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "first_steps_with_tensor_flow.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
